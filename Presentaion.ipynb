{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-06T23:14:18.225406900Z",
     "start_time": "2026-01-06T23:14:18.187333400Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T23:16:22.527682400Z",
     "start_time": "2026-01-06T23:14:18.197081100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import text_cleaner\n",
    "import filter_context\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import text_cleaner\n",
    "import ollama\n",
    "import os\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "import markdown2\n",
    "from flask import Flask, render_template, request\n",
    "import compare\n",
    "import gemenAI\n",
    "import markdown2  # for rendering markdown nicely\n",
    "import llama_model\n",
    "\n",
    "# Load a pre-trained embedding model (compact but powerful)\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ],
   "id": "13dfd3e537e84a37",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This cell is used for extracting the text from pdf using library pdf plubmber"
   ],
   "id": "131913f39cc9f02e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-06T23:16:22.549256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n"
   ],
   "id": "2669b9deb08a3fef",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This cell reads job description data from a JSON file, handling both line-delimited and standard JSON formats, converts\n",
    "the data into a pandas DataFrame, and then selects only the first 100 records for analysis."
   ],
   "id": "4dae4215e6664cad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T23:16:32.020001200Z",
     "start_time": "2026-01-06T23:16:22.559659200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_path = \"job-descriptions.json\"  # Update path\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        jobs_data = [json.loads(line) for line in f]\n",
    "except json.JSONDecodeError:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        jobs_data = json.load(f)\n",
    "jobs_df = pd.DataFrame(jobs_data)\n",
    "jobs_df = jobs_df.head(100)"
   ],
   "id": "ee5ccba58970bdae",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is a cell that removes unclean features from the job description text by renaming the column headers to normalized headers through the usage of the normalize function from the NLTK corpus. The cell will thereafter convert the cleaned features to numerical features through the calculation of the TF-IDF transform feature."
   ],
   "id": "35f7760e085a04af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T23:16:32.181432500Z",
     "start_time": "2026-01-06T23:16:31.544635200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# 4. Clearing the text and apply normalization from NLTK\n",
    "# -----------------------------\n",
    "jobs_df.columns = jobs_df.columns.str.strip().str.lower()\n",
    "jobs_df['clean_desc'] = jobs_df['description'].apply(filter_context.normalization)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5.Precompute TF-IDF matrix\n",
    "# -----------------------------\n",
    "job_texts = jobs_df['clean_desc'].tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "job_tfidf_matrix = vectorizer.fit_transform(job_texts)"
   ],
   "id": "9944c126d4831aa1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This tool matches a resume with job postings based on various similarity measures. The tool initializes the resume cleaning and vectorization process using TF-IDF similarity measures, and afterwards, transformer semantic embeddings are used. The tool further calculates keyword overlap between the resume and the job posting. The tool finally generates a hybrid score based on the above measures and produces the top matching job postings and the most important keyword phrases of the resume."
   ],
   "id": "6ae7a9d771e685a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T23:16:32.193838700Z",
     "start_time": "2026-01-06T23:16:32.177150900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_matches(resume_text, top_n_keywords=10, top_n_jobs=20):\n",
    "    # === 1. Clean + TF-IDF (existing NLTK logic) ===\n",
    "    resume_clean = filter_context.normalization(resume_text)\n",
    "    resume_keywords = filter_context.GetTFIDF(resume_clean, top_n=top_n_keywords)\n",
    "\n",
    "    resume_vec = vectorizer.transform([resume_clean])\n",
    "    similarity_scores_tfidf = cosine_similarity(resume_vec, job_tfidf_matrix)[0]\n",
    "    jobs_df['similarity_tfidf'] = similarity_scores_tfidf\n",
    "\n",
    "    # === 2. Semantic Embeddings (transformers) ===\n",
    "    resume_emb = semantic_model.encode(resume_clean, convert_to_tensor=True)\n",
    "    job_embs = semantic_model.encode(jobs_df['clean_desc'].tolist(), convert_to_tensor=True)\n",
    "    semantic_similarities = util.cos_sim(resume_emb, job_embs)[0].cpu().numpy()\n",
    "    jobs_df['similarity_semantic'] = semantic_similarities\n",
    "\n",
    "    # === 3. Keyword Overlap (from TF-IDF) ===\n",
    "    jobs_df['top_keywords'] = jobs_df['clean_desc'].apply(lambda x: filter_context.GetTFIDF(x, top_n=top_n_keywords))\n",
    "    jobs_df['keyword_overlap'] = jobs_df['top_keywords'].apply(\n",
    "        lambda x: len(set(resume_keywords).intersection(set(x)))\n",
    "    )\n",
    "\n",
    "    # === 4. Hybrid Combined Score ===\n",
    "    # You can tune these weights — semantic tends to be more robust\n",
    "    jobs_df['combined_score'] = (\n",
    "        0.3* jobs_df['similarity_tfidf']\n",
    "        + 0.4 * jobs_df['similarity_semantic']\n",
    "        + 0.3 * (jobs_df['keyword_overlap'] / top_n_keywords)\n",
    "    )\n",
    "\n",
    "    # === 5. Sort and Return ===\n",
    "    top_matches = jobs_df.sort_values(by='combined_score', ascending=False).head(top_n_jobs)\n",
    "\n",
    "    return resume_keywords, top_matches"
   ],
   "id": "8c65217bd48149ec",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It preprocesses the text for lower casing, removal of non-alphanumeric characters, tokenization, extraction of stopwords, and lemmatization to get the roots of words in the text. It also includes a function to dig into the grammatical structures using POS tagging for each sentence in the text, getting the frequency of every universal POS tag, and then returns a structured dataframe to show how linguistic categories are distributed in the text."
   ],
   "id": "64b083f0f3dfc245"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T23:16:32.224241300Z",
     "start_time": "2026-01-06T23:16:32.204634400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from filter_context import lemmatizer\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Clean, tokenize, remove stopwords, and lemmatize\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# -----------------------------\n",
    "# Get each usage for each Tag\n",
    "# -----------------------------\n",
    "def get_tag(text, tagset='universal'):\n",
    "    all_tags=['ADJ','ADP','ADV','CONJ','DET','NOUN','NUM','PRT','PRON','VERB','.','X']\n",
    "    rows = []\n",
    "\n",
    "    for sentence in text.split('.'):\n",
    "        pos_tags = Counter([j for i, j in nltk.pos_tag(word_tokenize(sentence), tagset=tagset)])\n",
    "        rows.append(pos_tags)\n",
    "\n",
    "    df = pd.DataFrame(rows).fillna(0).astype(int)\n",
    "\n",
    "    for col in all_tags:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    return df[all_tags]"
   ],
   "id": "358e4d036d6ff523",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It contains code that defines text normalization functions, keywords extraction functions, TF-IDF calculation, and finally returns the keywords representing the most meaningful part of the text. This is performed through text cleaning, removal of non-alphabetic letters, lemmatization, removal of stopwords, calculation of TF-IDF, sorting, and finally selecting the key words."
   ],
   "id": "9919175ec5c1392"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T23:16:32.225258300Z",
     "start_time": "2026-01-06T23:16:32.215450300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalization(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    text = text_cleaner.text_cleaner(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# -----------------------------\n",
    "# Get Top 20 keywords\n",
    "# -----------------------------\n",
    "def GetTFIDF(text, top_n=20):\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return []\n",
    "\n",
    "    cleaner_text = normalization(text)\n",
    "\n",
    "    docs = [cleaner_text]\n",
    "\n",
    "    vector_stop_words = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    tfidf_matrix = vector_stop_words.fit_transform(docs)\n",
    "    feature_names = np.array(vector_stop_words.get_feature_names_out())\n",
    "\n",
    "    scores = tfidf_matrix.toarray().flatten()\n",
    "\n",
    "    top_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    top_keyword = feature_names[top_indices]\n",
    "\n",
    "    return top_keyword.tolist()"
   ],
   "id": "12caddb3863e14b5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code sets up a Google Gemini API service by loading an API key from Environment Variables and also defines a function that constructs a response based on the Google Gemini model after implementing a retry feature in case of temporary server busy errors, so that it does not fail at the first attempt to respond."
   ],
   "id": "a32713db99972de7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T23:16:34.162785900Z",
     "start_time": "2026-01-06T23:16:32.226387800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from google import genai\n",
    "import os\n",
    "import google.genai.errors\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=api_key)\n",
    "# -----------------------------\n",
    "# Get Solution from the model\n",
    "# -----------------------------\n",
    "# We are using retries cause sometime google API can fail so at least we dont give error with first try\n",
    "def getSolution(prompt, retires=5):\n",
    "    for attempt in range(retires):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=prompt\n",
    "            )\n",
    "            generated_text = response.text\n",
    "\n",
    "            return generated_text\n",
    "        except google.genai.errors.ServerError as e:\n",
    "            if \"503\" in str(e):\n",
    "                wait = 2 ** attempt  # exponential backoff\n",
    "                print(f\"[WARN] Gemini overloaded, retrying in {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "    print(\"[ERROR] Gemini API still unavailable after retries.\")\n",
    "    return None\n",
    "\n",
    "\n"
   ],
   "id": "792b7b31c4f13ebf",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code defines utilities for generating a resume using a local LLaMA model by sending a prompt to Ollama and converting the model's response into a PDF file. It ensures that the output file has a unique name not to overwrite any existing file, transforms the generated markdown content into HTML, and saves the final formatted result as a PDF in the user's Downloads directory."
   ],
   "id": "40a19b2e3cf184e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T23:16:34.176960700Z",
     "start_time": "2026-01-06T23:16:34.169362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_unique_filename(directory, base_name, extension):\n",
    "    filename = f\"{base_name}{extension}\"\n",
    "    counter = 1\n",
    "\n",
    "    while os.path.exists(os.path.join(directory, filename)):\n",
    "        filename = f\"{base_name} ({counter}){extension}\"\n",
    "        counter += 1\n",
    "\n",
    "    return os.path.join(directory, filename)\n",
    "\n",
    "\n",
    "def useLlamaModel(prompt):\n",
    "    desiredModel = 'llama3.2:3b'\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=desiredModel,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    ollamaResponse = response['message']['content'] or \"\"\n",
    "\n",
    "    # Convert markdown → HTML\n",
    "    html = markdown2.markdown(ollamaResponse)\n",
    "\n",
    "    # Windows Downloads folder\n",
    "    downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "\n",
    "    # Generate unique filename\n",
    "    pdf_path = get_unique_filename(downloads_dir, \"Generated_Resume\", \".pdf\")\n",
    "\n",
    "    # Create PDF\n",
    "    doc = SimpleDocTemplate(pdf_path)\n",
    "    styles = getSampleStyleSheet()\n",
    "    story = [Paragraph(html, styles[\"Normal\"])]\n",
    "\n",
    "    doc.build(story)\n",
    "\n",
    "    return pdf_path"
   ],
   "id": "250b018ba2ee26e0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Below is the Flask app used as the backend of the resume evaluation and job matchmaking website. It enables the user to upload the resume in PDF form, download the text from the resume, use NLP-based resume matching algorithms to find appropriate job listings, as well as key words in the resume. The `/match_resume` path is used in the app to process the submitted resume, match the contents to the job descriptions in the database, and return the list of matching job listings to the front end. There is also the home path, which loads the application interface, initialized with default inputs. There is also an AI-based endpoint to enhance the submitted resume according to the instructions used."
   ],
   "id": "d63796fc8bebfc11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T23:20:11.525240200Z",
     "start_time": "2026-01-06T23:16:34.180201600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Reqeust to get matches for our CV and find strength and weakness\n",
    "# -----------------------------\n",
    "@app.route(\"/match-resume\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    resume_keywords = []\n",
    "    strengths_weaknesses_html = \"\"\n",
    "    matches = []\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        file = request.files.get(\"resume_pdf\")\n",
    "        if file:\n",
    "            # Extract resume text\n",
    "            resume_text = compare.extract_text_from_pdf(file)\n",
    "\n",
    "            # Compute keywords and top matches\n",
    "            resume_keywords, top_matches = compare.compute_matches(resume_text)\n",
    "\n",
    "            # Prepare prompt for GemenAI\n",
    "            # prompt = ', '.join(\n",
    "            #     resume_keywords) + \"\\n\" + resume_text + \"\\nCan you give strengths and weaknesses for this CV?\"\n",
    "            # strengths_weaknesses_md = gemenAI.getSolution(prompt)\n",
    "            #\n",
    "            # # Convert markdown to HTML\n",
    "            # strengths_weaknesses_html = markdown2.markdown(strengths_weaknesses_md)\n",
    "\n",
    "            # Convert matches to list of dicts\n",
    "            matches = top_matches.to_dict(orient=\"records\")\n",
    "\n",
    "#rendering the templates in this case our main html\n",
    "    return render_template(\n",
    "        \"main.html\",\n",
    "        resume_keywords=resume_keywords,\n",
    "        strengths_weaknesses_html=strengths_weaknesses_html,\n",
    "        matches=matches\n",
    "    )\n",
    "\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def home():\n",
    "    # Just render main.html with empty defaults\n",
    "    return render_template(\n",
    "        \"main.html\",\n",
    "        resume_keywords=[],\n",
    "        strengths_weaknesses_html=\"\",\n",
    "        matches=[]\n",
    "    )\n",
    "\n",
    "\n",
    "@app.route(\"/build-resume-ai\", methods=[\"POST\"])\n",
    "def build_resume_ai():\n",
    "    ai_result_html = \"\"\n",
    "\n",
    "    file = request.files.get(\"ai_resume_pdf\")\n",
    "    ai_prompt = request.form.get(\"ai_prompt\")\n",
    "\n",
    "    if file and ai_prompt:\n",
    "        resume_text = compare.extract_text_from_pdf(file)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Resume:\n",
    "        {resume_text}\n",
    "\n",
    "        Instruction:\n",
    "        {ai_prompt}\n",
    "\n",
    "        Please generate an improved professional resume.\n",
    "        \"\"\"\n",
    "\n",
    "        llama_model.useLlamaModel(prompt)\n",
    "\n",
    "    return render_template(\n",
    "        \"main.html\",\n",
    "        ai_result=ai_result_html\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ],
   "id": "52da40d5d1eb9d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [07/Jan/2026 00:16:59] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Jan/2026 00:17:44] \"POST /build-resume-ai HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Jan/2026 00:18:35] \"POST /build-resume-ai HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Jan/2026 00:19:31] \"POST /match-resume HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
