{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-30T18:27:35.149650Z",
     "start_time": "2025-12-30T18:27:35.145172Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:28:39.912994Z",
     "start_time": "2025-12-30T18:27:35.184071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import text_cleaner\n",
    "import filter_context\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import text_cleaner\n",
    "import ollama\n",
    "import os\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "import markdown2\n",
    "from flask import Flask, render_template, request\n",
    "import compare\n",
    "import gemenAI\n",
    "import markdown2  # for rendering markdown nicely\n",
    "import llama_model\n",
    "\n",
    "# Load a pre-trained embedding model (compact but powerful)\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ],
   "id": "13dfd3e537e84a37",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\klaus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\klaus\\PycharmProjects\\HackNation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "THis is For Faheritin",
   "id": "5da32ba2c1a45468"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:28:40.285964Z",
     "start_time": "2025-12-30T18:28:40.279141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n"
   ],
   "id": "2669b9deb08a3fef",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:28:48.720809Z",
     "start_time": "2025-12-30T18:28:40.292975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_path = \"job-descriptions.json\"  # Update path\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        jobs_data = [json.loads(line) for line in f]\n",
    "except json.JSONDecodeError:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        jobs_data = json.load(f)\n",
    "jobs_df = pd.DataFrame(jobs_data)\n",
    "jobs_df = jobs_df.head(100)"
   ],
   "id": "ee5ccba58970bdae",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:28:49.008780Z",
     "start_time": "2025-12-30T18:28:48.788376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# 4. Clearing the text and apply normalization from NLTK\n",
    "# -----------------------------\n",
    "jobs_df.columns = jobs_df.columns.str.strip().str.lower()\n",
    "jobs_df['clean_desc'] = jobs_df['description'].apply(filter_context.normalization)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5.Precompute TF-IDF matrix\n",
    "# -----------------------------\n",
    "job_texts = jobs_df['clean_desc'].tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "job_tfidf_matrix = vectorizer.fit_transform(job_texts)"
   ],
   "id": "9944c126d4831aa1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:28:49.052028Z",
     "start_time": "2025-12-30T18:28:49.037691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_matches(resume_text, top_n_keywords=10, top_n_jobs=20):\n",
    "    # === 1. Clean + TF-IDF (existing NLTK logic) ===\n",
    "    resume_clean = filter_context.normalization(resume_text)\n",
    "    resume_keywords = filter_context.GetTFIDF(resume_clean, top_n=top_n_keywords)\n",
    "\n",
    "    resume_vec = vectorizer.transform([resume_clean])\n",
    "    similarity_scores_tfidf = cosine_similarity(resume_vec, job_tfidf_matrix)[0]\n",
    "    jobs_df['similarity_tfidf'] = similarity_scores_tfidf\n",
    "\n",
    "    # === 2. Semantic Embeddings (transformers) ===\n",
    "    resume_emb = semantic_model.encode(resume_clean, convert_to_tensor=True)\n",
    "    job_embs = semantic_model.encode(jobs_df['clean_desc'].tolist(), convert_to_tensor=True)\n",
    "    semantic_similarities = util.cos_sim(resume_emb, job_embs)[0].cpu().numpy()\n",
    "    jobs_df['similarity_semantic'] = semantic_similarities\n",
    "\n",
    "    # === 3. Keyword Overlap (from TF-IDF) ===\n",
    "    jobs_df['top_keywords'] = jobs_df['clean_desc'].apply(lambda x: filter_context.GetTFIDF(x, top_n=top_n_keywords))\n",
    "    jobs_df['keyword_overlap'] = jobs_df['top_keywords'].apply(\n",
    "        lambda x: len(set(resume_keywords).intersection(set(x)))\n",
    "    )\n",
    "\n",
    "    # === 4. Hybrid Combined Score ===\n",
    "    # You can tune these weights — semantic tends to be more robust\n",
    "    jobs_df['combined_score'] = (\n",
    "        0.3* jobs_df['similarity_tfidf']\n",
    "        + 0.4 * jobs_df['similarity_semantic']\n",
    "        + 0.3 * (jobs_df['keyword_overlap'] / top_n_keywords)\n",
    "    )\n",
    "\n",
    "    # === 5. Sort and Return ===\n",
    "    top_matches = jobs_df.sort_values(by='combined_score', ascending=False).head(top_n_jobs)\n",
    "\n",
    "    return resume_keywords, top_matches"
   ],
   "id": "8c65217bd48149ec",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is for Arsilda",
   "id": "64b083f0f3dfc245"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:28:49.082079Z",
     "start_time": "2025-12-30T18:28:49.072929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from filter_context import lemmatizer\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Clean, tokenize, remove stopwords, and lemmatize\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# -----------------------------\n",
    "# Get each usage for each Tag\n",
    "# -----------------------------\n",
    "def get_tag(text, tagset='universal'):\n",
    "    all_tags=['ADJ','ADP','ADV','CONJ','DET','NOUN','NUM','PRT','PRON','VERB','.','X']\n",
    "    rows = []\n",
    "\n",
    "    for sentence in text.split('.'):\n",
    "        pos_tags = Counter([j for i, j in nltk.pos_tag(word_tokenize(sentence), tagset=tagset)])\n",
    "        rows.append(pos_tags)\n",
    "\n",
    "    df = pd.DataFrame(rows).fillna(0).astype(int)\n",
    "\n",
    "    for col in all_tags:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    return df[all_tags]"
   ],
   "id": "358e4d036d6ff523",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:28:49.114004Z",
     "start_time": "2025-12-30T18:28:49.104391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalization(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    text = text_cleaner.text_cleaner(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# -----------------------------\n",
    "# Get Top 20 keywords\n",
    "# -----------------------------\n",
    "def GetTFIDF(text, top_n=20):\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return []\n",
    "\n",
    "    cleaner_text = normalization(text)\n",
    "\n",
    "    docs = [cleaner_text]\n",
    "\n",
    "    vector_stop_words = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    tfidf_matrix = vector_stop_words.fit_transform(docs)\n",
    "    feature_names = np.array(vector_stop_words.get_feature_names_out())\n",
    "\n",
    "    scores = tfidf_matrix.toarray().flatten()\n",
    "\n",
    "    top_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    top_keyword = feature_names[top_indices]\n",
    "\n",
    "    return top_keyword.tolist()"
   ],
   "id": "12caddb3863e14b5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:28:50.566796Z",
     "start_time": "2025-12-30T18:28:49.134604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from google import genai\n",
    "import os\n",
    "import google.genai.errors\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=api_key)\n",
    "# -----------------------------\n",
    "# Get Solution from the model\n",
    "# -----------------------------\n",
    "# We are using retries cause sometime google API can fail so at least we dont give error with first try\n",
    "def getSolution(prompt, retires=5):\n",
    "    for attempt in range(retires):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=prompt\n",
    "            )\n",
    "            generated_text = response.text\n",
    "\n",
    "            return generated_text\n",
    "        except google.genai.errors.ServerError as e:\n",
    "            if \"503\" in str(e):\n",
    "                wait = 2 ** attempt  # exponential backoff\n",
    "                print(f\"[WARN] Gemini overloaded, retrying in {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "    print(\"[ERROR] Gemini API still unavailable after retries.\")\n",
    "    return None\n",
    "\n",
    "\n"
   ],
   "id": "792b7b31c4f13ebf",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This Part is For Klaus",
   "id": "40a19b2e3cf184e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:28:50.598920Z",
     "start_time": "2025-12-30T18:28:50.588424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_unique_filename(directory, base_name, extension):\n",
    "    filename = f\"{base_name}{extension}\"\n",
    "    counter = 1\n",
    "\n",
    "    while os.path.exists(os.path.join(directory, filename)):\n",
    "        filename = f\"{base_name} ({counter}){extension}\"\n",
    "        counter += 1\n",
    "\n",
    "    return os.path.join(directory, filename)\n",
    "\n",
    "\n",
    "def useLlamaModel(prompt):\n",
    "    desiredModel = 'llama3.2:3b'\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=desiredModel,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    ollamaResponse = response['message']['content'] or \"\"\n",
    "\n",
    "    # Convert markdown → HTML\n",
    "    html = markdown2.markdown(ollamaResponse)\n",
    "\n",
    "    # Windows Downloads folder\n",
    "    downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "\n",
    "    # Generate unique filename\n",
    "    pdf_path = get_unique_filename(downloads_dir, \"Generated_Resume\", \".pdf\")\n",
    "\n",
    "    # Create PDF\n",
    "    doc = SimpleDocTemplate(pdf_path)\n",
    "    styles = getSampleStyleSheet()\n",
    "    story = [Paragraph(html, styles[\"Normal\"])]\n",
    "\n",
    "    doc.build(story)\n",
    "\n",
    "    return pdf_path"
   ],
   "id": "250b018ba2ee26e0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:42:27.083790Z",
     "start_time": "2025-12-30T18:28:50.643945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Reqeust to get matches for our CV and find strength and weakness\n",
    "# -----------------------------\n",
    "@app.route(\"/match-resume\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    resume_keywords = []\n",
    "    strengths_weaknesses_html = \"\"\n",
    "    matches = []\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        file = request.files.get(\"resume_pdf\")\n",
    "        if file:\n",
    "            # Extract resume text\n",
    "            resume_text = compare.extract_text_from_pdf(file)\n",
    "\n",
    "            # Compute keywords and top matches\n",
    "            resume_keywords, top_matches = compare.compute_matches(resume_text)\n",
    "\n",
    "            # Prepare prompt for GemenAI\n",
    "            # prompt = ', '.join(\n",
    "            #     resume_keywords) + \"\\n\" + resume_text + \"\\nCan you give strengths and weaknesses for this CV?\"\n",
    "            # strengths_weaknesses_md = gemenAI.getSolution(prompt)\n",
    "            #\n",
    "            # # Convert markdown to HTML\n",
    "            # strengths_weaknesses_html = markdown2.markdown(strengths_weaknesses_md)\n",
    "\n",
    "            # Convert matches to list of dicts\n",
    "            matches = top_matches.to_dict(orient=\"records\")\n",
    "\n",
    "#rendering the templates in this case our main html\n",
    "    return render_template(\n",
    "        \"main.html\",\n",
    "        resume_keywords=resume_keywords,\n",
    "        strengths_weaknesses_html=strengths_weaknesses_html,\n",
    "        matches=matches\n",
    "    )\n",
    "\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def home():\n",
    "    # Just render main.html with empty defaults\n",
    "    return render_template(\n",
    "        \"main.html\",\n",
    "        resume_keywords=[],\n",
    "        strengths_weaknesses_html=\"\",\n",
    "        matches=[]\n",
    "    )\n",
    "\n",
    "\n",
    "@app.route(\"/build-resume-ai\", methods=[\"POST\"])\n",
    "def build_resume_ai():\n",
    "    ai_result_html = \"\"\n",
    "\n",
    "    file = request.files.get(\"resume_pdf\")\n",
    "    ai_prompt = request.form.get(\"ai_prompt\")\n",
    "\n",
    "    if file and ai_prompt:\n",
    "        resume_text = compare.extract_text_from_pdf(file)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Resume:\n",
    "        {resume_text}\n",
    "\n",
    "        Instruction:\n",
    "        {ai_prompt}\n",
    "\n",
    "        Please generate an improved professional resume.\n",
    "        \"\"\"\n",
    "\n",
    "        llama_model.useLlamaModel(prompt)\n",
    "\n",
    "    return render_template(\n",
    "        \"main.html\",\n",
    "        ai_result=ai_result_html\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ],
   "id": "52da40d5d1eb9d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [30/Dec/2025 19:42:21] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
